# Classic generataive neural network. This time, it generates audio.

module Autoencoder

using Flux: output_size
include("Globals.jl")
include("AudioIterator.jl")

using .AudioIterator, Flux, Serialization, WAV, Zygote
using  Distributions

# Model definition.

# A model is a series of nested functions that takes a multidimensional array as input, 
    # and keeps other multidimensional arrays as parameters (ex: kernels for the convolution function)

# These functions are implemented by the Flux package. 

# Most of everything is agnostic to this definition; as long as there are 5 components the rest of the program doesn't care what they are. 
# It would be simple to down/upsize. 

function create_model()

    encoder = Chain(

        # Encoding layer
    
        Conv( (3, 1), (2 => 4),  pad=2, stride=1), AdaptiveMeanPool( ( 4096 * 2, 1 ) ),
        Conv( (3, 1), (4 => 8),  pad=2, stride=2), AdaptiveMeanPool( ( 4096 - 1, 1 )  ),
        Conv( (3, 1), (8 => 16), pad=2, stride=2),

        AdaptiveMeanPool( ( output_shape , 1 ) ),
        Dropout(0.5) 
    )
    
    decoder = Chain(
    
        # Decoder layer

                                  ConvTranspose( (3, 1), 16 => 8, stride=2 ),
        Upsample( scale=(2, 1) ), ConvTranspose( (3, 1), 8 => 4,  stride=2 ),
        Upsample( scale=(2, 1) ), ConvTranspose( (3, 1), 4 => 2,  stride=2 ),
        
        AdaptiveMeanPool( ( sample_size, 1 ) )
    
    )
    
    mean        = Dense( output_shape, latent_vec_size )
    std         = Dense( output_shape, latent_vec_size )

    reconstruct = Upsample( size=( output_shape, 1) )

    return encoder, decoder, reconstruct, mean, std

end    

# Evaluates the output of the model given some input. 

# I tried to disentangle this as much as possible so that modifying this function 
    # modifies the actual mathematical actions undertaken by the model

    # i.e. this is separate from data parsing and backprop

    # That being said, this is probably the most resource intensive function. 

function eval_model( encoder, decoder, reconstruct, mean, std, param, data)

    # Encoding process
    enc_out    = encoder( data )

    # Learns the probability distribution of the output
    means      = mean( enc_out )
    devs       = std( enc_out )

    # Samples from the distribution generated by the encoder 
    latent     = ( param .* devs ) .+ means
    
    # Reconstructs the input 
    rec_out    = reconstruct( latent )
    dec_out    = decoder( rec_out )

    return enc_out, latent, dec_out

end

# The loss function and model evaluation that should be called within the backprop/gradient scope

function loss_function( encoder, decoder, reconstruct, mean, std, param, x )

    enc_out, rec_out, dec_out = eval_model( encoder, decoder, reconstruct, mean, std, param, x )
    
    return Flux.Losses.mse( dec_out, x ) , Flux.Losses.kldivergence( softmax( dec_out ), softmax( x ) )

end

# Runs a single training iteration with backprop. This function is executed the most. 

# I tried to disentangle this function from other aspects of the program, similarly to eval_model

    # However, it is more involved. Unlike eval_model, this function

        # Generates an array of random integers

        # Tracks model gradients

        # Prints to console 

        # Runs Flux's backprop

    # Actually evaluating the model and the backprop are most likely the most intensive parts 

function train_iter( data, model, opt, parameters )

    # Generates an array of random integers

    unit_gaussians = rand( Normal( 1.0, 0.1 ), latent_vec_size )

    r_loss, d_loss = 0, 0

    # Tracks model gradients

    gs = gradient( parameters ) do

        r_loss, d_loss = loss_function( model..., unit_gaussians, data )

        return 2 * r_loss + d_loss

    end

    sleep( 10.0 )

    # Prints to console 

    println('\n', "r ", string(r_loss)[1:7], " | d ", string(d_loss)[1:7] )

    # Runs Flux's backprop

    Flux.Optimise.update!( opt, parameters, gs )

end



# Checkpointing is achieved by 

    # serializing the model, its parameters, and the optimizer at its current state 

    # saving the number of iterations done on the current dataset

# The following functions implement this. 



# Saving is fairly cheap, since it's done infrequently. 

function save( count, model, parameters, opt )

    serialize( savename, ( count, model, parameters, opt ) )

end

# This function deserializes the model and relevant parameters from the disk, 
    # and sets the data IOPipe to the checkpointed position. 

# Loading is fairly cheap, since it's done infrequently.

function load()

    count, model, parameters, opt = deserialize( savename )

    io = open( data_file )

    deserialize( io )

    offset = position( io )

    seek( io, count * offset )

    return io, count, model, parameters, opt

end

# If we initialize the model to disk, we don't have to test for the initial case in train_iterations; 
    # i.e. we can blindly load() every call of train_iterations()

function init_model()

    count          = 0

    model = create_model()

    opt = ADAM( 0.01 )
    parameters = Flux.params( model[1:2]..., model[3:5] )

    serialize( savename, ( count, model, parameters, opt ) )

end

# This is the main driver for training. It is the most involved and encapsulating function, 
    # though it is separate from data parsing. 

# This function:

    # loads the last checkpoint

    # iterates 1 -> n, doing the following:

        # Checks if the loaded IOPipe is at its last position 

        # Deserializes data from the IOPipe

        # Does one iteration of training 

    # on completion, saves the model 

function train_iterations( num )

    io, count, model, parameters, opt = load()

    for i in 1:num

        if eof( io )

            count = 0
            seek( io, 0 )

            opt = ADAM( 0.01 )

        end

        data = deserialize( io )

        count = count + 1

        train_iter( data, model, opt, parameters )

        print( string(i / num)[1:4], " " )

    end

    save( count, model, parameters, opt )

end

# This is the main driver for doing inference. It runs on command, i.e. almost never. 

function autoencode( num_batches )

    _, model, _, _ = deserialize( savename )

    encoder, decoder, reconstruct, mean, std = model

    out = zeros( sample_size, 1, 2, batches, num_batches )

    io  = open( data_file )

    for i in 1 : num_batches 

        data = next( io )

        unit_gaussians = ones( latent_vec_size )

        _, _, output = eval_model( encoder, decoder, reconstruct, mean, std, unit_gaussians, data )

        out[:, :, :, :, i] = reshape( output, size(output)[ 1:4 ] )

    end
    
    file = reshape( out, ( sample_size * num_batches * batches, 2 ) )

    file = ( file .* 2.0 ) .- 1.0

    wavwrite( file, "output.wav", Fs=22050)

    close( io )

end

export init_model, train_iterations, autoencode

end